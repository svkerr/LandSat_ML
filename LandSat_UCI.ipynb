{
 "metadata": {
  "name": "LandSat_UCI"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LandSat Data Set obtained from University California Irvine"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using this data set, we will use SVM techniques to classify the type of imagery.\n",
      "\n",
      "Descriptions of this data set can be found in the file titled: `sat.doc`.\n",
      "\n",
      "Both the training and test files are space separated\n",
      "\n",
      "QUESTION: Should i scale `lsat_data`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn as sk\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsat = np.loadtxt('sat.trn.txt', delimiter = ' ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsat_tst = np.loadtxt('sat.tst.txt', delimiter = ' ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(lsat)\n",
      "print lsat.shape\n",
      "print lsat_tst.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'>\n",
        "(4435, 37)\n",
        "(2000, 37)\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsat_target = lsat[:,36]\n",
      "lsat_data = lsat[:,0:35]\n",
      "print lsat_target.shape\n",
      "print lsat_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(4435,)\n",
        "(4435, 35)\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ensure i captured just the last column for the target values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.max(lsat_target)\n",
      "print np.min(lsat_target)\n",
      "print np.mean(lsat_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7.0\n",
        "1.0\n",
        "3.65028184893\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SVC implementation (of an SVM) has various parameters. The most relevant is kernel, which defines the classifier's kernel (think of the kernel functions as different similarity measures between instances). The default SVC kernel is the rbf kernel, which allows us to model nonlinear problems. We will start with the simplest one: linear"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "svc_1 = SVC(kernel='linear')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,X_test, y_train, y_test = train_test_split(lsat_data, lsat_target, test_size = 0.25, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_train.shape\n",
      "print X_test.shape\n",
      "print y_train.shape\n",
      "print y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3326, 35)\n",
        "(1109, 35)\n",
        "(3326,)\n",
        "(1109,)\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, KFold\n",
      "from scipy.stats import sem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function to evaluate K-fold cross-validation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate_cross_validation(clf, X, y, K):\n",
      "    #create a k-fold cross validation iterator\n",
      "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
      "    # by default, the score used is the one returned by score method of estimator (`accuracy`)\n",
      "    scores = cross_val_score(clf, X, y, cv=cv)\n",
      "    print scores\n",
      "    print(\"Mean score: {0:.3f} (+/- {1:.3f})\").format(np.mean(scores),sem(scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_cross_validation(svc_1, X_train, y_train, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.87067669  0.86165414  0.87218045  0.86466165  0.84984985]\n",
        "Mean score: 0.864 (+/- 0.004)\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also define a function to perform training on the training set and evaluate the performance on the testing set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
      "    clf.fit(X_train,y_train)\n",
      "    print \"Accuracy on training set:\"\n",
      "    print clf.score(X_train, y_train)\n",
      "    print \"Accuracy on testing set:\"\n",
      "    print clf.score(X_test, y_test)\n",
      "    \n",
      "    y_pred = clf.predict(X_test)\n",
      "    \n",
      "    print \"Classification Report:\"\n",
      "    print metrics.classification_report(y_test, y_pred)\n",
      "    print \"Confusion Matrix\"\n",
      "    print metrics.confusion_matrix(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_and_evaluate(svc_1, X_train, X_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy on training set:\n",
        "0.90288634997"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on testing set:\n",
        "0.863841298467\n",
        "Classification Report:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.97      0.98      0.97       278\n",
        "          2       0.93      0.94      0.94       121\n",
        "          3       0.88      0.93      0.90       235\n",
        "          4       0.50      0.40      0.44        95\n",
        "          5       0.78      0.78      0.78       113\n",
        "          7       0.85      0.85      0.85       267\n",
        "\n",
        "avg / total       0.86      0.86      0.86      1109\n",
        "\n",
        "Confusion Matrix\n",
        "[[272   0   2   0   4   0]\n",
        " [  1 114   0   0   6   0]\n",
        " [  2   0 219  13   0   1]\n",
        " [  1   2  24  38   2  28]\n",
        " [  5   4   1   4  88  11]\n",
        " [  0   2   4  21  13 227]]\n"
       ]
      }
     ],
     "prompt_number": 50
    }
   ],
   "metadata": {}
  }
 ]
}